{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T15:49:42.323253Z","iopub.status.busy":"2024-06-09T15:49:42.322514Z","iopub.status.idle":"2024-06-09T15:49:54.795773Z","shell.execute_reply":"2024-06-09T15:49:54.794557Z","shell.execute_reply.started":"2024-06-09T15:49:42.323199Z"},"trusted":true},"outputs":[],"source":["pip install rouge_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","import gc\n","from rouge_score import rouge_scorer"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-09T14:43:35.803529Z","iopub.status.busy":"2024-06-09T14:43:35.803147Z","iopub.status.idle":"2024-06-09T14:43:35.815054Z","shell.execute_reply":"2024-06-09T14:43:35.814006Z","shell.execute_reply.started":"2024-06-09T14:43:35.803499Z"},"trusted":true},"outputs":[],"source":["for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T14:43:35.817479Z","iopub.status.busy":"2024-06-09T14:43:35.816754Z","iopub.status.idle":"2024-06-09T14:43:47.300065Z","shell.execute_reply":"2024-06-09T14:43:47.298956Z","shell.execute_reply.started":"2024-06-09T14:43:35.817454Z"},"trusted":true},"outputs":[],"source":["input_files = [\n","    '/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-1.txt',\n","    '/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-2.txt',\n","    '/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-3.txt',\n","    '/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-4.txt',\n","    '/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-5.txt',\n","    '/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-6.txt',\n","    '/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-7.txt',\n","    '/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-8.txt',\n","    '/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-9.txt'\n","]\n","\n","output_file = '/kaggle/working/persian_wikipedia.txt'\n","\n","with open(output_file, 'w', encoding='utf-8') as outfile:\n","    for fname in input_files:\n","        with open(fname, 'r', encoding='utf-8') as infile:\n","            outfile.write(infile.read())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PersianWikiDataset(Dataset):\n","    def __init__(self, text_file, sequence_length, subset_size=None):\n","        self.sequence_length = sequence_length\n","        with open(text_file, 'r', encoding='utf-8') as f:\n","            self.text = f.read()\n","\n","        if subset_size:\n","            self.text = self.text[:subset_size]\n","        \n","        self.chars = sorted(list(set(self.text)))\n","        self.char_to_idx = {char: idx for idx, char in enumerate(self.chars)}\n","        self.idx_to_char = {idx: char for idx, char in enumerate(self.chars)}\n","\n","        self.encoded_text = [self.char_to_idx[char] for char in self.text]\n","\n","    def __len__(self):\n","        return len(self.encoded_text) - self.sequence_length\n","\n","    def __getitem__(self, idx):\n","        x = self.encoded_text[idx:idx + self.sequence_length]\n","        y = self.encoded_text[idx + 1:idx + self.sequence_length + 1]\n","        return torch.tensor(x), torch.tensor(y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TextGeneratorModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n","        super(TextGeneratorModel, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, x, hidden):\n","        x = self.embedding(x)\n","        out, hidden = self.lstm(x, hidden)\n","        out = self.fc(out.reshape(out.size(0) * out.size(1), out.size(2)))\n","        return out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n","                  weight.new(self.num_layers, batch_size, self.hidden_dim).zero_())\n","        return hidden"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def clear_memory():\n","    gc.collect()\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(model, dataloader, epochs, lr, accumulation_steps=4):\n","    model.train()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","    model.zero_grad()\n","\n","    for epoch in range(epochs):\n","        hidden = model.init_hidden(dataloader.batch_size)\n","        hidden = tuple([each.to(device) for each in hidden])\n","        \n","        for i, (x, y) in enumerate(dataloader):\n","            if x.size(0) != hidden[0].size(1):  # Check batch size consistency\n","                hidden = model.init_hidden(x.size(0))\n","                hidden = tuple([each.to(device) for each in hidden])\n","            \n","            x, y = x.to(device), y.to(device)  # Move data to GPU\n","            hidden = tuple([each.data.to(device) for each in hidden])\n","            output, hidden = model(x, hidden)\n","            loss = criterion(output, y.view(-1))\n","            loss.backward()\n","            \n","            if (i + 1) % accumulation_steps == 0:\n","                optimizer.step()\n","                model.zero_grad()\n","\n","            # Clear memory periodically\n","            clear_memory()\n","\n","        print(f'Epoch: {epoch + 1}/{epochs}, Loss: {loss.item()}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_model(model, dataloader):\n","    model.eval()\n","    total_loss = 0\n","    criterion = nn.CrossEntropyLoss()\n","    with torch.no_grad():\n","        for x, y in dataloader:\n","            x, y = x.to(device), y.to(device)\n","            hidden = model.init_hidden(x.size(0))  # Initialize hidden state with current batch size\n","            hidden = tuple([each.to(device) for each in hidden])\n","            output, hidden = model(x, hidden)\n","            loss = criterion(output, y.view(-1))\n","            total_loss += loss.item()\n","    \n","    perplexity = torch.exp(torch.tensor(total_loss / len(dataloader)))\n","    return perplexity.item()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Use a smaller subset for initial testing\n","subset_size = 100000  # Use 1 million characters for example\n","sequence_length = 30  # Shorter sequence length\n","text_file = '/kaggle/working/persian_wikipedia.txt'\n","dataset = PersianWikiDataset(text_file, sequence_length, subset_size=subset_size)\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True)  # Smaller batch size"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize model and move to device\n","vocab_size = len(dataset.chars)\n","embedding_dim = 128\n","hidden_dim = 128  # Reduced hidden dimension\n","num_layers = 1    # Reduced number of layers\n","model = TextGeneratorModel(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T14:43:47.302400Z","iopub.status.busy":"2024-06-09T14:43:47.302069Z","iopub.status.idle":"2024-06-09T15:49:42.320988Z","shell.execute_reply":"2024-06-09T15:49:42.319937Z","shell.execute_reply.started":"2024-06-09T14:43:47.302373Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1/5, Loss: 1.7002850770950317\n","Epoch: 2/5, Loss: 1.5726206302642822\n","Epoch: 3/5, Loss: 1.9303953647613525\n","Epoch: 4/5, Loss: 1.3571174144744873\n","Epoch: 5/5, Loss: 1.3596223592758179\n","Perplexity: 4.25123929977417\n"]}],"source":["# Train model\n","epochs = 5\n","lr = 0.001\n","train_model(model, dataloader, epochs, lr)\n","\n","# Evaluate model\n","perplexity = evaluate_model(model, dataloader)\n","print(f'Perplexity: {perplexity}')"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-09T15:49:54.798561Z","iopub.status.busy":"2024-06-09T15:49:54.798235Z","iopub.status.idle":"2024-06-09T15:49:56.641594Z","shell.execute_reply":"2024-06-09T15:49:56.640634Z","shell.execute_reply.started":"2024-06-09T15:49:54.798534Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'rouge1': Score(precision=0.5, recall=0.3333333333333333, fmeasure=0.4), 'rougeL': Score(precision=0.5, recall=0.3333333333333333, fmeasure=0.4)}\n"]}],"source":["# Generate some text using the model and compare it to a reference text\n","reference = \"some reference text\"\n","predicted = \"generated text\"\n","rouge_scores = compute_rouge(predicted, reference)\n","print(rouge_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_rouge(predicted, reference):\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n","    scores = scorer.score(reference, predicted)\n","    return scores"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":836206,"sourceId":1427869,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
